{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable: UC_DATA_ROOT has been set to default: /home/alexv84/Documents/GitHub/streamlit/data\n",
      "Environment Variable: UC_CODE_DIR has been set to default: /home/alexv84/Documents/GitHub/streamlit/src\n",
      "Environment Variable: UC_PROFILE has been set to default: prod\n",
      "Environment Variable: UC_OPENAI_API_KEY has been set to default: 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\n",
      "CSV Service read from file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/Claim descr.csv\n",
      "(200000, 42)\n",
      "(200000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de1_eks_postext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monatgelohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kontruktionskleber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      de1_eks_postext\n",
       "0         Monatgelohn\n",
       "1  Kontruktionskleber"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import count, groupby\n",
    "from tokenize import group\n",
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from copy import deepcopy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk, re, string, typing        # for type hints\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from copy import deepcopy\n",
    "from importlib import reload\n",
    "from src.utils import utils as util\n",
    "from src.services import file\n",
    "from src.config import global_config as glob\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "reload(glob)\n",
    "reload(util)\n",
    "reload(file)\n",
    "\n",
    "# js = file.JSONservice(child_path='data')\n",
    "# german_stopwords = js.doRead(filename='stopwords.json')\n",
    "\n",
    "file_name = \"Claim descr.csv\"\n",
    "\n",
    "csv = file.CSVService(path=file_name, root_path=Path.home() / \"Documents/Arbeit/Allianz/AZVers\", delimiter=\",\")\n",
    "\n",
    "df = csv.doRead()\n",
    "print(df.shape)\n",
    "\n",
    "#df.head(1000)\n",
    "#df.info(verbose=True)\n",
    "\n",
    "#col_sel = ['id_sch','invoice_item_id', 'dl_gewerk','firma', 'yylobbez', 'erartbez', 'hsp_eigen', 'hsp_prodbez', 'sartbez', 'sursbez', 'schilderung', 'de1_eks_postext']\n",
    "#col_sel = ['dl_gewerk','de1_eks_postext']\n",
    "col_sel = ['de1_eks_postext']\n",
    "\n",
    "corpus = df[col_sel]#.head(1*10**5)\n",
    "\n",
    "print(corpus.shape)\n",
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 232 stop words.\n",
      "Adding custom German stop words...\n",
      "Added 351 stopword(s).\n",
      "Removed 2 stopword(s).\n",
      "Finished preprocessing.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import HashingVectorizer   # use integer hash instead of actual token in memory\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reload(util)\n",
    "#reload(glob)\n",
    "\n",
    "#target = LabelEncoder().fit_transform(corpus['dl_gewerk'].tolist())   # labels\n",
    "X = corpus['de1_eks_postext']\n",
    "#corpus['target'] = target\n",
    "\n",
    "cleaner = util.clean_text(language='german', without_stopwords=['nicht', 'keine'])\n",
    "\n",
    "X_cl = cleaner.fit_transform(X)\n",
    "\n",
    "#docs = X_cl.tolist()\n",
    "#target_names = corpus['dl_gewerk'].tolist()       # class labels\n",
    "\n",
    "corpus_cl = X_cl.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "sentences = corpus_cl.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, sg=0, hs=1, window=4, min_count=1, workers=4, epochs=80)   # CBOW ; min_count = 1 needed otherwise infrequent tokens are not ebing assigned a word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=86048, vector_size=100, alpha=0.025>\n",
      "86048\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "vocab_len = len(model.wv)\n",
    "print(vocab_len)\n",
    "\n",
    "#rock_idx = model.wv.key_to_index[\"rock\"]\n",
    "#rock_cnt = model.wv.get_vecattr(\"rock\", \"count\") \n",
    "\n",
    "vocab = model.wv.index_to_key    # used vocabulary\n",
    "\n",
    "#random_word = random.choice(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector = model.wv['reinigung']  # get numpy vector of a word\n",
    "#sims = model.wv.most_similar('reinigung', topn=10)  # get other similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectors = np.asarray(model.wv.vectors)\n",
    "#vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=cleaner.stop_words)\n",
    "\n",
    "#tf_idf = vectorizer.fit_transform(X_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1722e36ea4a49c19dfbb4d5f007233a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200000, 100)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "doc_vectors = np.empty((len(sentences),model.vector_size))\n",
    "\n",
    "for i, doc in enumerate(tqdm(sentences, total=len(sentences))):\n",
    "    vec=np.empty((model.vector_size,0))    # empty column\n",
    "    if len(doc)>0:\n",
    "        for token in doc:\n",
    "            vector = model.wv[token]\n",
    "            vec = np.column_stack((vec, vector))\n",
    "        doc_vectors[i,:] = np.nanmean(vec, axis=1)\n",
    "\n",
    "doc_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.any(np.isnan(doc_vectors))\n",
    "#np.all(np.isfinite(doc_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embedding = umap.UMAP(n_components=15).fit(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = umap.plot.points(embedding, labels=hover_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('streaml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2792acb2c2eca0081b1fbec2b68cca76691c24070c4d0a29e9fe2b09cce74dcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
